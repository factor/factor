* Definition of generic words
- Generic word has =def>>= slot which contains quotation to a call to =mega-cache-lookup=.
- This definition quotation is called when passed to the non-optimizing compiler
  non-optimizing compiler and thus the =mega-cache-lookup= call is encountered
  in =quotation_jit::iterate_quotation=, which compiles in a primitive call via
  =emit_mega_cache_lookup=:
  - emits a call to =PIC-LOAD= (which I think is the PIC call stub, and in the
    default implementation simply gets the dispatch#-th argument from the stack)
  - emits a call to =MEGA-LOOKUP=.  At this point, if there was a cache hit,
    the following instructions are not executed
  - emits a stack frame for:
    - push method table
    - push dispatch#
    - push the cache
    - a call to the miss word, which ends up in =primitve_megacache_lookup=

* Lookup/Engine compilation
- entry point: =build-decision-tree=
  - gets the =methods= word prop, which is an assoc from classes to methods,
    calls =<engine>=:
    - This is set up to always generate a tag dispatch engine
      For single combination does :
      - =flatten-methods=: This is the first transformation step, which expands
        the assoc to cover union classes. Now we have an assoc

        #+begin_src factor
          { { class1 T{ predicate-engine f participant1 { M\ class1 method ... } } } }
        #+end_src

        This ensures that predicate engines always dispatch on built-in or tuple
        classes (as per the return value of =flatten-class=), effectively
        distributing the methods over the union.

      - Next step is =convert-tuple-methods=.  This rebalances all tuple classes
        under the =tuple= key, and calls =<tuple-dispatch-engine>= on that
        subtree, creating the correctly nested =<echelon-dispatch-engine>=
        structure internally.  The original leaf engine nodes are untouched,
        e.g. there are still =T{ predicate-engine }= entries there.


** TODO Modification for nested lookup
- Introduce =nested-dispatch-engine= engine, which is like =tag-dispatch-engine=, but
  takes a position argument and compiles into a word like =predicate-engine= when compiled.
- Modify =flatten-method= to work on a multi-dispatch tree, compiling
  =nested-dispatch-engine= instead of =predicate-engine= whenever an assoc is
  encountered instead of a single method.
  - TODO also define =pic-def*= on that

* TODO PIC
- two types of caches: =PIC_TAG= and =PIC_TUPLE=, the type is determined by
  cache contents alone in =determine_inline_cache_type=.  A cold cache starts
  out as =PIC_TAG= cache.
- PIC code blocks generated by the inline cache jit get the type
  =CODE_BLOCK_PIC=.  This is checked during relocation.  Ops in these code
  blocks are not subject to =RT_ENTRY_POINT_PIC*=-type relocations.
- Still question: ??? How the front is a PIC instance actually generated?
  - Looks to me right now that almost every word call is emitted using
    =JIT_WORD_CALL= or =JIT_WORD_JUMP=, which define relocations that will
    reference the pic-entry-point when possible. -> THIS MEANS:?! Any word with
    a pic-def property could be cached using the inline dispatch cache on the parameter???.
  - When the call to the cache quotation results in a cache update, the assembly
    code changes at the call site.  This hints at the stub being completely
    replaced by a jump into the cache tests.
    - TODO: how is the space for the inline cache quotations allocated?
      Actually, on a miss, the whole jit is invoked, allocates a new code block
      and initializes it, then based on the return address the call address is modified.
** =PIC_TAG= Inline Cache
- contains jit-compiled successive checks =PIC_CHECK_TAG= which compare the loaded thing
  (=PIC_LOAD=) with the cache entry keys by simply comparing the object's tag.
** =PIC_TUPLE= Inline Cache
- instead of checking the tag, this generates code to CMP the two registers
  containing the loaded object and the cache entry key.
** TODO Difference
It seems like the difference is that the TAG check performs an immediate
comparison, while the tuple check has to perform an additional load???

I think the idea is that if dispatch is done only on built-in classes, the tag
comparison is sufficient, while in the case of tuple classes, the pointer to the
layout has to be compared?

The object to check itself is encoded as tagged object.  If comparing e.g.
fixnums, then the tag must be compared.  If it is a tuple, the value part of the
object is a pointer to the layout.  Thus, when comparing tuple classes, the
layout field of the class has to be fetched for comparison.

??? But does that not mean that if the cache type is set to =PIC_TUPLE=, a
fixnum could then never be a hit? (or only if it is the exact number?)

* Compilation of generic word calls
** Non-optimizing compiler
*** Generic word definition
- The generic word has been defined with the =pic-def>>= and =pic-tail-def>>=
  slots which contain factor code defining the inline cache stubs.  These are of
  the form =[ \ word { ... dispatch-table ... } dispatch# { } inline-cache-miss ]=
- When finishing a compilation unit, the word is passed to =recompile=.  Since
  it is not to be optimized, the quotation will be passed to =modify-code-heap=,
  i.e. =primitive_modify_code_heap=, which will invoke =jit_compile_word=
- Like for regular words, the =def>>= quotation is jit-compiled via
  =jit_compile_quotation=
- Additionally, the same thing is done to the =pic-def>>= and =pic-tail-def>>=
  slots for the generic word
*** Generic word call site
- =primitive_modify_code_heap= → =update_code_heap_words= →
  =update_word_references= is called on the complete code heap, which iterates
  over (TODD: what exactly???) some instruction-operands of a code-block, where
  it dispatches on a relocation type
- In addition to =RT_ENTRY_POINT=, which seems to compute and return the entry
  point of the thing being called, there is =RT_ENTRY_POINT_PIC= and
  =RT_ENTRY_POINT_PIC_TAIL=, which call =compute_entry_point_pic_{tail_}address=
- That in turn returns the entry point of the compiled code in the =pic-def>>= slot
- This can be seen via =disassemble= of a word containing a generic call site,
  which references the corresponding =pic-def= or =pic-tail-def= quotation.

* Lookup-Method procedure in VM,

Entry point: =lookup-method= primitive in =generic.single.private=, implemented
in vm as =primitive_lookup_method=.  Either called directly from code, or via a
mega-cache miss inside the VM

- pop methods dispatch table, and object, call =lookup_method(obj, methods)=
- =lookup_method=
  - Index the dispatch table by the object tag -> =method=
  - If =obj= is a tuple, call =lookup_tuple_method=, where =method= is the
    tuple dispatch array
  - Otherwise, return =method=
  - =lookup_tuple_method=
    - get the tuple =layout= from =obj=
    - untag the =methods= array as =echelons=
    - initialize echelon number =echelon= as
       =min(layout->echelon, length(echelons)-1)=, basically starting at the
      most-specific subclass possible.  This already rules out arguments which
      are more specific
    - While the root echelon has not been reached:
      - get the element at position =echelon= from =echelons= -> =echelon-methods=
      - if =echelon-methods= is a word, return it. (I think that means that the
        dispatch is delegated to a word, e.g. a predicate engine)
      - otherwise, if the entry is not =f=, it is a hash-table:
        - set =klass= to the nth superclass, which is at offset =2*echelon= in
          the layout
        - get the corresponding nth superclass =hashcode= (offset =2*echelon+1=
          in the layout)
        - perform a lookup in that hashtable via =search_lookup_hash=, which
          extracts the corresponding bucket via hash-code bit-masking.  If the
          element is an array, perform an alist-search for the =klass=,
          otherwise it was a non-ambiguous hit, so the element itself is the
          method. (Note: it seems that arrays are always used, even for
          non-ambiguous hits.  Note 2, )
      - If none of the above steps have returned, decrease =echelon= and repeat.
    - If nothing has been found at =echelon == 0=, the lookup failed.  This is
      an error.

Interpretation: The lookup procedure is always =call(lookup(class,
hashtable(echelon)))=, with decreasing echelons until the lookup is not =f=.
This means we have for one argument position a linear superclass search
followed by a log hash-table lookup as basic means of operation.

??? What I don't quite get is why there could be empty echelon tables?  Isn't,
at least for single dispatch, the correct thing to call clear already at every
echelon, if the most-specific one has been ruled out?  (Is there some kind of
space/time tradeoff there?)

Answer: It is kept sparse.  The methods are only stored in the echelons which
their specializer resides in.

* Modification for multiple dispatch lookup
The procedure described above performs a search for =klass= in the table, which
is a sequence of dispatch tables.  =echelon= is the index to which table is
current.  This can be interpreted as an automaton with state transfer and
lookup.  In this case, the transfer function will return the next upper echelon,
predicated on whether the correct result has been returned.

It should be possible by turning this into a state machine suitable for one of
the DFA lookup methods by augmenting it with a second element to return from the
element, which can be used as the selector for the next echelon table to test.

This will also include instructions on which class to test next.  This is
straightforward if we turn =klass= into an array, and the current test index
becomes part of the state.

This turns the whole thing into a simple automaton described by an array of
state transfer functions.

There are several possibilities of what runtime tests are necessary.  The  most
general one seems to be predicate-based, which encompasses the more practically
relevant cases.  Factor uses class equality only.

** Class equality-based
Pros:
- Cheapest low-level test (pointer comparison).

Cons:
- Probably large overhead in creating the FSM
  - No subclass range comparisons means that there will probably be a lot of
    states (compared to subclass tests)
  - Every added subclass will probably always trigger more table recompilations

** Subclass-based
Pros and cons opposite to above.  Specifically, the subclass test is normally
quite expensive.  However, the echelon structure of two layouts should provide
that result in constant time.

* Covariant tuple dispatch

This is the default "mode" of Julia-style signature semantics.
Without diagonal dispatch (type variables and matching), this boils down to an
ordering where e.g. if Int < Number, { Int, String } < { Number, String }.  This
seems to be the default semantics for most generic dispatch systems.  CLOS-style
systems impose a lexicographic ordering precedence in addition.

** Implementation idea for dispatch
Consider the case for Rock < Thing, Paper < Thing , Scissors < Thing, and
additionally The-Rock < Rock, all tuple classes.

Assume the following methods for a generic function
- m1(Rock, Paper)
- m2(Scissors, Rock)
- m3(Paper, Scissors)
- m4(Thing, Thing)
- m5(Thing, The-Rock)


Tests work by performing comparing the class.  A dispatch table is generated for
each upper echelon in single dispatch case
At runtime, each object can be tested for class equality with one memory access, and
for subclass membership with two memory accesses and an offset computation (in
addition to the numerical comparison).

*** Method Sorting
Without taking into account ambiguous cases for now, because of the covariant
property, sort methods into deepest-class first.  The length of the inheritance
chain is attached to both objects and classes directly in Factor.

This corresponds to sorting according to echelons, but taking the maximum of
every argument.

For the example above:
1. m5(Thing, The-Rock) (1,3)
2. These have the same maximum echelon (2,2)
   - m1(Rock, Paper)
   - m2(Scissors, Rock)
   - m3(Paper, Scissors)
3. m4(Thing, Thing) (1,1)

*** Decision Tree generation (optimize for number of caches visited per lookup)
Approach: Generate decision tree for each argument position in turn, choosing
the argument position which can be used to cut down the most cases first, to
make use of the PIC for that position.

In this example, checking for the second parameter #0 first, because it cuts the
search space most efficiently?
- Sort by specificity on #0,then #1: (m1,m2,m3,m5,m4)

- (m5, m1, m2, m3, m4)
  - #0 echelons
    1.
       - Thing :: (m5, m4)
         - #1 echelons
           1. Thing (m4)
           2. f
           3. The-Rock (m5)
    2.
       - Rock :: (m1,m5,m4)
         - #1 echelons
           1. Thing (m4)
           2. Paper (m1)
           3. The-Rock (m5)
       - Scissors :: (m2,m5,m4)
         - #1 echelons
           1. Thing (m4)
           2. Rock (m2)
           3. The-Rock (m5)
       - Paper :: (m3, m5, m4)
         - #1 echelons
           1. Thing (m4)
           2. Scissors (m3)
           3. The-Rock (m5)

Whenever there is a choice with more than one method remaining, a dispatch word
must be provided, which then generates the next dispatch test. TODO: Check if we
can also store quotations directly?
